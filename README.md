# LLM  
**ะะพะปะปะตะบัะธั ะธ ัะตะฐะปะธะทะฐัะธั Large Language Models ั ะฝัะปั**

ะญัะพั ัะตะฟะพะทะธัะพัะธะน ะฟัะตะดะฝะฐะทะฝะฐัะตะฝ ะดะปั ัะตะฐะปะธะทะฐัะธะธ, ะพะฑััะตะฝะธั ะธ ัะบัะฟะตัะธะผะตะฝัะพะฒ ั ัะฐะทะปะธัะฝัะผะธ ะฐััะธัะตะบัััะฐะผะธ **Large Language Models (LLM)**.  
ะฆะตะปั ะฟัะพะตะบัะฐ โ **ะณะปัะฑะพะบะพะต ะฟะพะฝะธะผะฐะฝะธะต ะฒะฝัััะตะฝะฝะตะณะพ ััััะพะนััะฒะฐ LLM**, ะฐ ะฝะต ะฟัะพััะพ ะธัะฟะพะปัะทะพะฒะฐะฝะธะต ะณะพัะพะฒัั ะฑะธะฑะปะธะพัะตะบ.

ะะฐะถะดะฐั ะผะพะดะตะปั ัะตะฐะปะธะทัะตััั:
- ะฒ ะพัะดะตะปัะฝะพะน ะฟะฐะฟะบะต,
- ั ัะพะฑััะฒะตะฝะฝะพะน ะฐััะธัะตะบัััะพะน,
- ัะฒะพะธะผ ะพะฑััะฐััะธะผ ะบะพะดะพะผ,
- ัะฒะพะธะผ ัะพะบะตะฝะธะทะฐัะพัะพะผ ะธ ัะตะบะฟะพะธะฝัะฐะผะธ.

ะะตะฟะพะทะธัะพัะธะน ะผะฐัััะฐะฑะธััะตััั: ะฝะพะฒัะต ะผะพะดะตะปะธ ะฑัะดัั ะดะพะฑะฐะฒะปััััั ะฟะพััะตะฟะตะฝะฝะพ (GPT-2, LLaMA, Mistral, Mixtral, Gemma ะธ ะดั.).

---

## ๐ ะะฑัะฐั ััััะบัััะฐ ัะตะฟะพะทะธัะพัะธั
```text
llm/
โโโ README.md
โโโ requirements.txt                                    
โโโ models/
    โโโ gpt1/                  # ะะตะฐะปะธะทะฐัะธั GPT-1
    โ   โโโ checkpoints/       # ะกะพััะฐะฝัะฝะฝัะต ะฒะตัะฐ ะผะพะดะตะปะตะน
    โ   โ   โโโ gpt_checkpoint.pt
    โ   โโโ src/
    โ       โโโ bpe/           # Byte Pair Encoding ัะพะบะตะฝะธะทะฐัะพั
    โ       โ   โโโ __init__.py
    โ       โ   โโโ bpe.py
    โ       โโโ model/         # ะััะธัะตะบัััะฐ ะผะพะดะตะปะธ
    โ       โ   โโโ __init__.py
    โ       โ   โโโ gpt.py
    โ       โโโ scripts/       # ะะฑััะตะฝะธะต ะธ ะณะตะฝะตัะฐัะธั
    โ           โโโ __init__.py
    โ           โโโ train.py
    โ           โโโ generate.py
    โ
    โโโ gpt2/                  # (ะฟะปะฐะฝะธััะตััั)
    โโโ llama/                 # (ะฟะปะฐะฝะธััะตััั)
    โโโ mistral/               # (ะฟะปะฐะฝะธััะตััั)
    โโโ mixtral/               # (ะฟะปะฐะฝะธััะตััั)
    โโโ gemma/                 # (ะฟะปะฐะฝะธััะตััั)
```


ะะฐะถะดะฐั ะผะพะดะตะปั ะฟะพะปะฝะพัััั **ะธะทะพะปะธัะพะฒะฐะฝะฐ** ะธ ะฝะต ะทะฐะฒะธัะธั ะพั ะดััะณะธั.

---

# ะะตะฐะปะธะทะพะฒะฐะฝะฝัะต ะผะพะดะตะปะธ

---

<details>
<summary><strong> GPT-1 </strong></summary>

### ะััะธัะตะบัััะฐ

- Decoder-only Transformer
- Masked Multi-Head Self-Attention
- Residual connections
- LayerNorm
- FeedForward ะฑะปะพะบะธ
- ะะปะฐััะธัะตัะบะฐั GPT-ะปะพะณะธะบะฐ (next-token prediction)

---

### ๐ ะกัััะบัััะฐ `models/gpt1`
**ะะฐัะฐัะตั:** [Russian Novels](https://github.com/JoannaBy/RussianNovels/tree/master)
```text
models/gpt1/
โโโ checkpoints/
โ โโโ gpt_checkpoint.pt # ะฒะตัะฐ ะผะพะดะตะปะธ
โโโ src/
โโโ bpe/
โ โโโ init.py
โ โโโ bpe.py # ัะตะฐะปะธะทะฐัะธั Byte Pair Encoding
โโโ model/
โ โโโ init.py
โ โโโ gpt.py # ะผะพะดะตะปั GPT + Dataset + train/generate
โโโ scripts/
โโโ init.py
โโโ train.py # ะพะฑััะตะฝะธะต ะผะพะดะตะปะธ
โโโ generate.py # ะณะตะฝะตัะฐัะธั ัะตะบััะฐ
```

---

### ๐งพ ะะฐะฝะฝัะต

- `tokenizer.json`  
  ะกะพะดะตัะถะธั:
  - `token2id`
  - `id2token`
  - `vocab_size`

- `token_ids.pt`  
  ะะดะธะฝ ะดะปะธะฝะฝัะน ัะตะฝะทะพั ัะพะบะตะฝะพะฒ ะฒัะตะณะพ ะบะพัะฟััะฐ  
  ะัะฟะพะปัะทัะตััั ะดะปั ัะพัะผะธัะพะฒะฐะฝะธั train / validation ะฒัะฑะพัะพะบ

---

### ๐ ะะฑััะตะฝะธะต

ะะฐะฟััะบ ะพะฑััะตะฝะธั ะธะท ะบะพัะฝั ัะตะฟะพะทะธัะพัะธั:

```bash
python -m models.gpt1.src.scripts.train
```
โจ ะะตะฝะตัะฐัะธั ัะตะบััะฐ
```bash
python -m models.gpt1.src.scripts.generate
```
โ๏ธ ะัะธะผะตั ะฟะฐัะฐะผะตััะพะฒ ะพะฑััะตะฝะธั

| ะะฐัะฐะผะตัั      | ะะฝะฐัะตะฝะธะต      |
| ------------- | ------------- |
| vocab_size    | 2000โ3000     |
| seq_len       | 64โ256        |
| emb_size      | 256โ512       |
| num_heads     | 4โ8           |
| num_layers    | 4โ12          |
| dropout       | 0.1โ0.2       |
| learning_rate | 1e-5 โ 2.5e-4 |
| batch_size    | 32โ128        |

</details>

<details>
<summary><strong> GPT-2 </strong></summary>

### ะััะธัะตะบัััะฐ

- Decoder-only Transformer
- Masked Multi-Head Self-Attention
- KV-cache (Key / Value caching)
- Residual connections
- LayerNorm (Pre-LN)
- FeedForward ะฑะปะพะบะธ (Linear โ GELU โ Linear)
- next-token prediction
---

### ๐ ะกัััะบัััะฐ `models/gpt2`
**ะะฐัะฐัะตั:** [Russian Novels](https://github.com/JoannaBy/RussianNovels/tree/master)
```text
models/gpt2/
โโโ checkpoints/
โ   โโโ gpt2_checkpoint.pt        # ะฒะตัะฐ ะผะพะดะตะปะธ
โโโ data/
โ   โโโ corpus/                   # ะธััะพะดะฝัะต ัะตะบััั 
โโโ src/
โ   โโโ bpe/
โ   โ   โโโ bpe.py                # ัะตะฐะปะธะทะฐัะธั BPE
โ   โ   โโโ tokenizer_generate.py # ะพะฑััะตะฝะธะต ัะพะบะตะฝะฐะนะทะตัะฐ
โ   โโโ model/
โ   โ   โโโ __init__.py
โ   โ   โโโ activations.py        # GELU
โ   โ   โโโ gpt2.py               # ะผะพะดะตะปั GPT-2 + KV-cache
โ   โโโ scripts/
โ       โโโ encode_corpus.py      # ะบะพะดะธัะพะฒะฐะฝะธะต ะบะพัะฟััะฐ ะฒ token_ids.pt
โ       โโโ train.py              # ะพะฑััะตะฝะธะต ะผะพะดะตะปะธ
โ       โโโ generate.py           # ะณะตะฝะตัะฐัะธั ัะตะบััะฐ
```

---

### ๐งพ ะะฐะฝะฝัะต

- `tokenizer.json`  
  ะกะพะดะตัะถะธั:
  - `token2id`
  - `id2token`
  - `vocab_size`

- `token_ids.pt`  
  ะะดะธะฝ ะดะปะธะฝะฝัะน ัะตะฝะทะพั ัะพะบะตะฝะพะฒ ะฒัะตะณะพ ะบะพัะฟััะฐ  
  ะัะฟะพะปัะทัะตััั ะดะปั ัะพัะผะธัะพะฒะฐะฝะธั train / validation ะฒัะฑะพัะพะบ

---

### ๐ ะะฑััะตะฝะธะต

ะะฐะฟััะบ ะพะฑััะตะฝะธั ะธะท ะบะพัะฝั ัะตะฟะพะทะธัะพัะธั:

```bash
python -m models.gpt2.src.scripts.train
```
โจ ะะตะฝะตัะฐัะธั ัะตะบััะฐ
```bash
python -m models.gpt2.src.scripts.generate
```
โ๏ธ ะัะธะผะตั ะฟะฐัะฐะผะตััะพะฒ ะพะฑััะตะฝะธั

| ะะฐัะฐะผะตัั      | ะะฝะฐัะตะฝะธะต          |
| ------------- | ----------------- |
| vocab_size    | ะธะท tokenizer.json |
| seq_len       | 128 โ 256         |
| emb_size      | 256               |
| num_heads     | 4 โ 8             |
| num_layers    | 4 โ 8             |
| dropout       | 0.1               |
| learning_rate | 3e-4              |
| batch_size    | 32 โ 64           |

</details>
